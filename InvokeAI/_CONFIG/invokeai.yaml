InvokeAI:
  Web Server:
    host: 192.168.1.250
    port: 9090
    allow_origins: []
    allow_credentials: true
    allow_methods:
    - '*'
    allow_headers:
    - '*'
    ssl_certfile: null
    ssl_keyfile: null
  Features:
    esrgan: true
    internet_available: true
    log_tokenization: false
    patchmatch: true
    ignore_missing_core_models: false
  Paths:
    autoimport_dir: G:\GenAI-data\InvokeAI-3-models\autoimport
    lora_dir: G:\GenAI-data\InvokeAI-3-models\autoimport\lora
    embedding_dir: G:\GenAI-data\InvokeAI-3-models\autoimport\embedding
    controlnet_dir: G:\GenAI-data\InvokeAI-3-models\autoimport\controlnet
    conf_path: configs\models.yaml
    models_dir: models
    legacy_conf_dir: configs\stable-diffusion
    db_dir: E:\Documents\GenerativeAI\InvokeAI-3.6.2\databases
    outdir: E:\Documents\GenerativeAI\InvokeAI-3.6.2
    use_memory_db: false
    custom_nodes_dir: nodes
  Logging:
    log_handlers:
    - console
    log_format: color
    log_level: info
    log_sql: false
  Development:
    dev_reload: false
  Model Cache:
    ram: 9.0
    vram: 4.0
    lazy_offload: true
    log_memory_usage: false
  Device:
    device: cuda
    precision: auto
  Generation:
    sequential_guidance: false
    attention_type: auto
    attention_slice_size: auto
    force_tiled_decode: false
    png_compress_level: 6
  Queue:
    max_queue_size: 10000
  Nodes:
    allow_nodes: null
    deny_nodes: null
    node_cache_size: 512
